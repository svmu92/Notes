Memory management tasks:-
- Management of physical pages in memory.
- The buddy system to allocate memory in large chunks.
- The slab, slub, and slob allocators to allocate smaller chunks of memory.
- The vmalloc mechanism to allocate non-contiguous blocks of memory.
- The address space of processes.


- The virtual address space of the processor is in general divided into two parts by
the Linux kernel. The lower and larger part is available to user processes, and the upper part is
reserved for the kernel. Whereas the lower part is modified during a context switch (between two
user processes), the kernel part of virtual address space always remains the same.

- On IA-32 systems,
the address space is typically divided between user processes and the kernel in a ratio of 3 : 1;
given a virtual address space of 4 GiB, 3 GiB would be available to userspace and 1 GiB for the
kernel.

- The virtual address space portion of the kernel is necessarily smaller than the
maximum theoretical address space of the CPU.

- On IA-32 systems, up to 896 MiB of RAM can be managed directly; anything
above this figure (up to a maximum of 4 GiB) can only be addressed by means of highmem.

- Highmem mode is not required on 64-bit machines because the available address space is gigantic, even
if physical addressing is limited to a smaller number of bits, for example, 48 or 52.

- The use of highmem pages is problematic only for the kernel itself. The kernel
must first invoke the kmap and kunmap functions discussed below to map the
highmem pages into its virtual address space before it can use them — this is not
necessary with normal memory pages. However, for userspace processes, it makes
absolutely no difference if the pages are highmem or normal pages because they are
always accessed via page tables and never directly.


- There are two types of machine that manage physical memory in different ways:

  UMA machines (uniform memory access) organize available memory in a contiguous fashion
  (possibly with small gaps). Each processor (in a symmetric multiprocessor system) is able to
  access each memory area equally quickly.

  NUMA machines (non-uniform memory access) are always multiprocessor machines. Local
  RAM is available to each CPU of the system to support particularly fast access. The proces-
  sors are linked via a bus to support access to the local RAM of other CPUs — this is naturally
  slower than accessing local RAM.
  Examples of such systems are Alpha-based WildFire servers and NUMA-Q machines
  from IBM.


- The kernel distinguishes three configuration options — FLATMEM , DISCONTIGMEM , and SPARSEMEM .
SPARSEMEM and DISCONTIGMEM serve practically the same purpose, but in the view of developers, differ in
the quality of their code — SPARSEMEM is regarded as more experimental and less stable but does feature
performance optimizations. Discontiguous memory is presumed to be more stable, but is not prepared
for new features like memory hotplugging.

- FLATMEM memory organization type is used on most configurations and is also usually the kernel default.

- Real NUMA systems will set the configuration option CONFIG_NUMA , and the memory management
codes will differ between the two variants. Since the flat memory model will not make sense on NUMA
machines, only discontiguous and sparse memory will be available.



allocation order - denotes
the binary logarithm of the number of pages that are contained in a memory region.


Organization in the (N)UMA Model
================================

- The kernel uses identical data structures for machines with uniform and non-uniform memory access so
that the individual algorithms need make little or no distinction between the various forms of memory
arrangement. On UMA systems, a single NUMA node is introduced to help manage the entire system
memory. The other parts of memory management are led to believe that they are working with a pseudo-
NUMA system.


- First, RAM memory is divided into nodes. A node is associated with each processor of the system and is
represented in the kernel by an instance of "pg_data_t".
"""
* On NUMA machines, each NUMA node would have a pg_data_t to describe
* it's memory layout. On UMA machines there is a single pglist_data which
* describes the whole memory.
*
* Memory statistics and page replacement data structures are maintained on a
* per-zone basis.
*/
typedef struct pglist_data {
       /*
[...]
      /* Per-node vmstats */
      struct per_cpu_nodestat __percpu *per_cpu_nodestats;
      atomic_long_t           vm_stat[NR_VM_NODE_STAT_ITEMS];
} pg_data_t;
"""

- Each node is split into "zones" as further subdivisions of memory.

- There are restrictions as
to the memory area that can be used for DMA operations (with ISA devices); only the first 16 MiB are
suitable. There is also a highmem area that cannot be mapped directly. Between these is the ‘‘normal‘‘
memory area for universal use. A node therefore comprises up to three zones.

- The kernel introduces the following constants to enumerate all zones in the system:

<linux/mmzone.h>
"""
enum zone_type {
	/*
	 * ZONE_DMA and ZONE_DMA32 are used when there are peripherals not able
	 * to DMA to all of the addressable memory (ZONE_NORMAL).
	 * On architectures where this area covers the whole 32 bit address
	 * space ZONE_DMA32 is used. ZONE_DMA is left for the ones with smaller
	 * DMA addressing constraints. This distinction is important as a 32bit
	 * DMA mask is assumed when ZONE_DMA32 is defined. Some 64-bit
	 * platforms may need both zones as they support peripherals with
	 * different DMA addressing limitations.
	 */
#ifdef CONFIG_ZONE_DMA
	ZONE_DMA,
#endif
#ifdef CONFIG_ZONE_DMA32
	ZONE_DMA32,
#endif
	/*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
	ZONE_NORMAL,
#ifdef CONFIG_HIGHMEM
	/*
	 * A memory area that is only addressable by the kernel through
	 * mapping portions into its own address space. This is for example
	 * used by i386 to allow the kernel to address the memory beyond
	 * 900MB. The kernel will set up special mappings (page
	 * table entries on i386) for each page that the kernel needs to
	 * access.
	 */
	ZONE_HIGHMEM,
#endif
	/*
	 * ZONE_MOVABLE is similar to ZONE_NORMAL, except that it contains
	 * movable pages with few exceptional cases described below. Main use
	 * cases for ZONE_MOVABLE are to make memory offlining/unplug more
	 * likely to succeed, and to locally limit unmovable allocations - e.g.,
	 * to increase the number of THP/huge pages. Notable special cases are:
	 *
   [...]
	 *
	 * In general, no unmovable allocations that degrade memory offlining
	 * should end up in ZONE_MOVABLE. Allocators (like alloc_contig_range())
	 * have to expect that migrating pages in ZONE_MOVABLE can fail (even
	 * if has_unmovable_pages() states that there are no unmovable pages,
	 * there can be false negatives).
	 */
	ZONE_MOVABLE,
#ifdef CONFIG_ZONE_DEVICE
	ZONE_DEVICE,
#endif
	__MAX_NR_ZONES

};
"""

- ZONE_DMA for DMA-suitable memory. The size of this region depends on the processor type. ON
IA-32 machines, the limit is the classical 16 MiB boundary imposed by ancient ISA devices. But
also, more modern machines can be affected by this.

- ZONE_DMA32 for DMA-suitable memory in a 32-bit addressable area. Obviously, there is only a
difference between the two DMA alternatives on 64-bit systems. On 32-bit machines, this zone
is empty; that is, its size is 0 MiB. On Alphas and AMD64 systems, for instance, this zone ranges
from 0 to 4 GiB.

- ZONE_NORMAL for normal memory mapped directly in the kernel segment. This is the only zone
guaranteed to be possible present on all architectures. It is, however, not guaranteed that the
zone must be equipped with memory. If, for instance, an AMD64 system has 2 GiB of RAM, then
all of it will belong to ZONE_DMA32 , and ZONE_NORMAL will be empty.

- ZONE_HIGHMEM for physical memory that extends beyond the kernel segment.

- Depending on the compile-time configuration, some zones need not be considered.
64-bit systems, for instance, do not require a high memory zone, and the DMA32 zone
is only required on 64-bit systems that also support 32-bit peripheral devices that
can only access memory up to 4 GiB.

- The kernel additionally defines a pseudo-zone ZONE_MOVABLE , which is required when efforts are made to
prevent fragmentation of the physical memory.

- __MAX_NR_ZONES acts as an end marker if the kernel wants to iterate over all zones present in the system.

- Each zone is associated with an array in which the physical memory pages belonging to the
zone — known as page frames in the kernel — are organized. An instance of "struct page" with the
required management data is allocated for each page frame.

- The nodes are kept on a singly linked list so that the kernel can traverse them.

 - For performance reasons, the kernel always attempts to perform the memory allocations of a process on
the NUMA node associated with the CPU on which it is currently running.

- Each node provides a fallback list (with the help of struct zonelist).
The list contains other nodes (and associated zones) that can be
used as alternatives for memory allocation. The further back an entry is on the list, the less suitable it is.

RAM ---> NUMA nodes (pg_data_t) ---> zones (struct zones) ---> page frames(struct page)



Node Management
---------------

- "pg_data_t" is the base element used to represent a node

<linux/mmzone.h>
"""
typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];

	struct zonelist node_zonelists[MAX_ZONELISTS];

	int nr_zones; /* number of populated zones in this node */
#ifdef CONFIG_FLATMEM	/* means !SPARSEMEM */
	struct page *node_mem_map;
#ifdef CONFIG_PAGE_EXTENSION
	struct page_ext *node_page_ext;
#endif
#endif
#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
	spinlock_t node_size_lock;
#endif
	unsigned long node_start_pfn;
	unsigned long node_present_pages; /* total number of physical pages */
	unsigned long node_spanned_pages; /* total size of physical page
					     range, including holes */
	int node_id;
	wait_queue_head_t kswapd_wait;
	wait_queue_head_t pfmemalloc_wait;
	struct task_struct *kswapd;	/* Protected by
					   mem_hotplug_begin/end() */
	int kswapd_order;
	enum zone_type kswapd_highest_zoneidx;

	int kswapd_failures;		/* Number of 'reclaimed == 0' runs */

#ifdef CONFIG_COMPACTION
	int kcompactd_max_order;
	enum zone_type kcompactd_highest_zoneidx;
	wait_queue_head_t kcompactd_wait;
	struct task_struct *kcompactd;
	bool proactive_compact_trigger;
#endif

	unsigned long		totalreserve_pages;

#ifdef CONFIG_NUMA
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
#endif /* CONFIG_NUMA */

	/* Write-intensive fields used by page reclaim */
	ZONE_PADDING(_pad1_)

#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
	unsigned long first_deferred_pfn;
#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */

#ifdef CONFIG_TRANSPARENT_HUGEPAGE
	struct deferred_split deferred_split_queue;
#endif

	struct lruvec		__lruvec;

	unsigned long		flags;

	ZONE_PADDING(_pad2_)

	/* Per-node vmstats */
	struct per_cpu_nodestat __percpu *per_cpu_nodestats;
	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];
} pg_data_t;
"""

"""
[ 6715.131748] Size of pg_data_t = 171968
"""


- "node_zones" is an array that holds the data structures of the zones in the node.
- "node_zonelists" specifies alternative nodes and their zones in the order in which they are used
for memory allocation if no more space is available in the current zone.
- The number of different zones in the node is held in "nr_zones".
- "node_mem_map" is a pointer to an array of page instances used to describe all physical pages of the
node. It includes the pages of all zones in the node.
- "node_start_pfn" is the logical number of the first page frame of the NUMA node. The page
frames of all nodes in the system are numbered consecutively, and each frame is given a number
that is globally unique (not just unique to the node).
    "node_start_pfn" is always 0 in a UMA system because there is only one node whose first page
    frame is therefore 0.
- "node_present_pages" specifies the number of page frames in the zone.
- "node_spanned_pages" the size of the zone in page frames. This value need not necessarily be the
same as "node_present_pages" because there may be holes in the zone that are not backed by a
real page frame.
- "node_id" is a global node identifier. All NUMA nodes in the system are numbered starting
from 0.
- "kswapd_wait" is the wait queue for the swap daemon needed when swapping frames out of the
zone.
- "kswapd" points to the task structure of the swap daemon responsible for the zone.
- "kswapd_max_order" is used in the implementation of the swapping
subsystem to define the size of the area to be freed.

- The zones of the node are held in "node_zones[MAX_NR_ZONES]" . The array always has three entries, even
if the node has fewer zones. If the latter is the case, the remaining entries are filled with null elements.


Node state management
---------------------
- If more than one node can be present on the system, the kernel keeps a bitmap that provides state infor-
mation for each node. The states are specified with a bitmask, and the following values are possible:

<linux/nodemask.h>
"""
/*
 * Bitmasks that are kept for all the nodes.
 */
enum node_states {
	N_POSSIBLE,		/* The node could become online at some point */
	N_ONLINE,		/* The node is online */
	N_NORMAL_MEMORY,	/* The node has regular memory */
#ifdef CONFIG_HIGHMEM
	N_HIGH_MEMORY,		/* The node has regular or high memory */
#else
	N_HIGH_MEMORY = N_NORMAL_MEMORY,
#endif
	N_MEMORY,		/* The node has memory(regular, high, movable) */
	N_CPU,		/* The node has one or more cpus */
	N_GENERIC_INITIATOR,	/* The node has one or more Generic Initiators */
	NR_NODE_STATES
};
"""

- The states N_POSSIBLE , N_ONLINE , and N_CPU are required for CPU and memory hotplugging.

- Essential for memory management are the flags N_HIGH_MEMORY
and N_NORMAL_MEMORY . While the first one announces that the zone is equipped with memory that may
be either regular or high memory, N_NORMAL_MEMORY is only set if non-highmem memory is present
on a node.

- Two auxiliary functions are provided to set or clear, respectively, a bit in the bit-field or a specific node:
"""
static inline void node_set_state(int node, enum node_states state)
{
	__node_set(node, &node_states[state]);
}

static inline void node_clear_state(int node, enum node_states state)
{
	__node_clear(node, &node_states[state]);
}
"""

- The macro "for_each_node_state(__node, __state)"" allows for iterating over all nodes
that are in a specific state.
- The macro "for_each_online_node(node)"" iterates over all active nodes.

- If the kernel is compiled to support only a single node, that is, using the flat memory model, the node
bitmap is not present, and the functions to manipulate it resolve to empty operations that simply do
nothing.
