BPF is a highly flexible and efficient virtual machine-like construct in the Linux kernel allowing to execute bytecode at various hook points in a safe manner. It is used in a number of Linux kernel subsystems, most prominently networking, tracing and security (e.g. sandboxing).


BPF Architecture
================
- own instruction set
- maps which act as efficient key/value stores.
- helper functions to interact with and leverage kernel functionality.
- tail calls for calling into other eBPF programs.
- security hardening primitives
- a psuedo filesystem for pinning objects (maps, programs),
- infrastruture for allowing BPF to be loaded.



Instruction set
---------------
- general purpose RISC instuction set.
- originally designed for the purpose of writing programs in a subset of C which can be compiled into BPF instructions through a compiler back end (e.g. LLVM), so that the kernel can later on map them through an in-kernel JIT compiler into native opcodes for optimal execution performance inside the kernel.

The advantages of pushing these instuctions into the kernel include:
- Making the kernel programmable without having to cross kernel / user space boundaries. State between BPF programs and kernel/user space can still be shared through maps whenever needed.

- Programs can be heavily optimized for performance by compiling out features that are not required.

- BPF programs can be updated automatically without having to restart the kernel, system services or containers.

- BPF provides a stable ABI towards user space, and doesn't require any third party kernel modules.

- BPF programs are portable across different architectures.

- BPF programs make use of the existing kernel infrastructure (e.g. drivers, netdevices, tunnels, protocol stack, sockets) and tooling (e.g. iproute2) as well as the safety guarantees which the kernel provides. 

- BPF programs are verified through an in-kernel verifier in order to ensure that they cannot crash the kernel, always terminate, etc.



The execution of a BPF program inside the kernel is always event-driven! Examples:
- A networking device which has a BPF program attached on its ingress path will trigger the execution of the program once a packet is received.
- A kernel address which has a kprobe with a BPF program attached will trap once the code at that address gets executed, which will then invoke the kprobe’s callback function for instrumentation, subsequently triggering the execution of the attached BPF program.



- eleven(11) 64-bit registers with 32-bit subregisters - r0 to r10.
- a Program counter (PC)
- a 512-byte large BPF stack space.

- 64-bit operating mode by default. 32-bit subregisters can only be accessed through special ALU instructions.

- r10 is read-only. Contains the frame pointer address to access the BPF stack space.
- r0-r9 are read/write general purpose.

- A BPF program can call into a predefined helper function, which is defined by the core kernel.

The BPF calling convention is defined as follows:
- r0 contains the return value of a helper function call.
- r1-r5 hold arguments from the BPF program to the kernel helper function.
- r6-r9 are callee saved registers that will be preserved on helper function call.

- Since the calling convention maps directly to x86_64, arm64 and other ABIs, JIT only needs to issue a call instruction, but no additional extra moves for placing function arguments. All BPF registers map one to one to HW CPU registers.

- Calls with 6 or more arguments are currently not supported.

- The helper functions in the kernel which are dedicated to BPF (BPF_CALL_0() to BPF_CALL_5() functions) are specifically designed with this convention in mind.

- r0 also holds the exit value of the BPF program. When handing execution back to the kernel, the exit value is passed as a 32 bit value.

- Registers r1 - r5 are scratch registers, meaning the BPF program needs to either spill them to the BPF stack or move them to callee saved registers if these arguments are to be reused across multiple helper function calls. Spilling means that the variable in the register is moved to the BPF stack. The reverse operation of moving the variable from the BPF stack to the register is called filling. The reason for spilling/filling is due to the limited number of registers.

- Upon entering execution of a BPF program, register r1 initially contains the context for the program. The context is the input argument for the program (similar to argc/argv pair for a typical C program). BPF is restricted to work on a single context. The context is defined by the program type, for example, a networking program can have a kernel representation of the network packet (skb) as the input argument.

- The maximum instruction limit per program is restricted to 4096 BPF instructions. For kernel newer than 5.1 this limit was lifted to 1 million BPF instructions. 

- Although the instruction set contains forward as well as backward jumps, the in-kernel BPF verifier will forbid loops so that termination is always guaranteed. 

- There is also a concept of tail calls that allows for one BPF program to jump into another one. This, too, comes with an upper nesting limit of 33 calls


- The instruction format is modeled as two operand instructions, which helps mapping BPF instructions to native instructions during JIT phase.

- The instruction set is of fixed size, meaning every instruction has 64 bit encoding.

- Currently, 87 instructions have been implemented and the encoding also allows to extend the set with further instructions when needed.

- The instruction encoding of a single 64 bit instruction on a big-endian machine is defined as a bit sequence from most significant bit (MSB) to least significant bit (LSB) of 
	>>>>> op:8, dst_reg:4, src_reg:4, off:16, imm:32 <<<<

- off and imm are of signed type.

- The encodings are part of the kernel headers and defined in linux/bpf.h header, which also includes linux/bpf_common.h.

/* BPF has 10 general purpose 64-bit registers and stack frame. */
#define MAX_BPF_REG	__MAX_BPF_REG

struct bpf_insn {
	__u8	code;		/* opcode */
	__u8	dst_reg:4;	/* dest register */
	__u8	src_reg:4;	/* source register */
	__s16	off;		/* signed offset */
	__s32	imm;		/* signed immediate constant */
};

- "op" - actual operation to be performed
- The operation can be based on register or immediate operands. The encoding of op itself provides information on which mode to use 
BPF_X for denoting register-based operations, and BPF_K for immediate-based operations respectively.

- Both "dst_reg" and "src_reg" provide additional information about the register operands to be used (e.g. r0 - r9) for the operation.

- "off" is used in some instructions to provide a relative offset, for example, for addressing the stack or other buffers available to BPF (e.g. map values, packet data, etc), or jump targets in jump instructions. 

- "imm" contains a constant / immediate value.




op - Operation:

- The available "op" instructions can be categorized into various instruction classes. These classes are also encoded inside the op field.
- The "op" field is divided into (from MSB to LSB) "code:4", "source:1" and "class:3". 
	"class" is the more generic instruction class, 
	"code" denotes a specific operational code inside that "class", and 
	"source" tells whether the source operand is a register or an immediate value.

- Possible instruction classes include:

1) BPF_LD, BPF_LDX: Both classes are for load operations. 
	BPF_LD is used for loading a double word as a special instruction spanning two instructions due to the imm:32 split, and for byte / half-word / word loads of packet data.  
	BPF_LDX class holds instructions for byte / half-word / word / double-word loads out of memory. Memory in this context is generic and could be stack memory, map value data, packet data, etc.
	
2) BPF_ST, BPF_STX: Both classes are for store operations.
	BPF_STX is the store counterpart and is used to store the data from a register into memory, which, again, can be stack memory, map value, packet data, etc. BPF_STX also holds special instructions for performing word and double-word based atomic add operations.
	The BPF_ST class is similar to BPF_STX by providing instructions for storing data into memory only that the source operand is an immediate value.

3) BPF_ALU, BPF_ALU64: Both classes contain ALU operations. 
	Both ALU classes have basic operations with source operand which is register-based and an immediate-based counterpart.
	BPF_ALU - 32-bit
	BPF_ALU64 - 64-bit	
	Supported by both are add (+), sub (-), and (&), or (|), left shift (<<), right shift (>>), xor (^), mul (*), div (/), mod (%), neg (~) operations. Also mov (<X> := <Y>) was added as a special ALU operation for both classes in both operand modes. BPF_ALU64 also contains a signed right shift. BPF_ALU additionally contains endianness conversion instructions for half-word / word / double-word on a given source register.
	
4) BPF_JMP: This class is dedicated to jump operations. 
	Unconditional jumps simply move the program counter forward, so that the next instruction to be executed relative to the current instruction is "off + 1", where "off" is the constant offset encoded in the instruction. Since "off" is signed, the jump can also be performed backwards as long as it does not create a loop and is within program bounds.
	Conditional jumps operate on both, register-based and immediate-based source operands. If the condition in the jump operations results in true, then a relative jump to off + 1 is performed, otherwise the next instruction (0 + 1) is performed. 
	Available conditions are jeq (==), jne (!=), jgt (>), jge (>=), jsgt (signed >), jsge (signed >=), jlt (<), jle (<=), jslt (signed <), jsle (signed <=) and jset (jump if DST & SRC). 
	Apart from that, there are three special jump operations within this class: 
		- the exit instruction which will leave the BPF program and return the current value in r0 as a return code,
		- the call instruction, which will issue a function call into one of the available BPF helper functions, 
		- a hidden tail call instruction, which will jump into a different BPF program.


- The Linux kernel is shipped with a BPF interpreter which executes programs assembled in BPF instructions. Currently x86_64, arm64, ppc64, s390x, mips64, sparc64 and arm architectures come with an in-kernel eBPF JIT compiler.

- All BPF handling such as loading of programs into the kernel or creation of BPF maps is managed through a central "bpf()" system call. It is also used for managing map entries (lookup / update / delete), and making programs as well as maps persistent in the BPF file system through pinning.





Helper Functions
----------------
- enables BPF programs to consult a core kernel defined set of function calls in order to retrieve / push data from / to the kernel.
- available helper functions may differ for each BPF program type

- Each helper function is implemented with a commonly shared function signature similar to system calls. The signature is defined as:
	u64 fn(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)

- The kernel abstracts helper functions into macros BPF_CALL_0() to BPF_CALL_5() which are similar to those of system calls. 

- Example of a helper function which updates map elements by calling into the corresponding map implementation callbacks:

"""

BPF_CALL_4(bpf_map_update_elem, struct bpf_map *, map, void *, key,
           void *, value, u64, flags)
{
    WARN_ON_ONCE(!rcu_read_lock_held());
    return map->ops->map_update_elem(map, key, value, flags);
}

const struct bpf_func_proto bpf_map_update_elem_proto = {
    .func           = bpf_map_update_elem,
    .gpl_only       = false,
    .ret_type       = RET_INTEGER,
    .arg1_type      = ARG_CONST_MAP_PTR,
    .arg2_type      = ARG_PTR_TO_MAP_KEY,
    .arg3_type      = ARG_PTR_TO_MAP_VALUE,
    .arg4_type      = ARG_ANYTHING,
};

"""

- Each newly added helper function will be JIT compiled in a transparent and efficient way, meaning that the JIT compiler only needs to emit a call instruction since the register mapping is made in such a way that BPF register assignments already match the underlying architecture’s calling convention. This allows for easily extending the core kernel with new helper functionality. 
- All BPF helper functions are part of the core kernel and cannot be extended or added through kernel modules.

"""
include/linux/bpf.h


/* eBPF function prototype used by verifier to allow BPF_CALLs from eBPF programs
 * to in-kernel helper functions and for adjusting imm32 field in BPF_CALL
 * instructions after verifying
 */
struct bpf_func_proto {
	u64 (*func)(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
	bool gpl_only;
	bool pkt_access;
	enum bpf_return_type ret_type;
	enum bpf_arg_type arg1_type;
	enum bpf_arg_type arg2_type;
	enum bpf_arg_type arg3_type;
	enum bpf_arg_type arg4_type;
	enum bpf_arg_type arg5_type;
};

"""
- The above "struct bpf_func_proto" is used to hand all the necessary information which need to be known about the helper to the verifier, so that the verifier can make sure that the expected types from the helper match the current contents of the BPF program’s analyzed registers.

- Argument types can range from passing in any kind of value up to restricted contents such as a pointer / size pair for the BPF stack buffer, which the helper should read from or write to. In the latter case, the verifier can also perform additional checks, for example, whether the buffer was previously initialized.

- List of available helper functions = all of BPF_CALL_0() - BPF_CALL_5() functions.

- The kernel’s "struct bpf_verifier_ops" contains a "get_func_proto" callback function that provides the mapping of a specific enum "bpf_func_id" to one of the available helpers for a given BPF program type.




Maps
------
- Maps are efficient key / value stores that reside in kernel space. 
- They can be accessed from a BPF program in order to keep state among multiple BPF program invocations. 
- They can also be accessed through file descriptors from user space and can be arbitrarily shared with other BPF programs or user space applications.
- A single BPF program can currently access up to 64 different maps directly.
- Different program types can access same maps - maps can be shared.

- Map implementations are provided by the core kernel. 
- There are generic maps with per-CPU and non-per-CPU flavor that can read / write arbitrary data, but there are also a few non-generic maps that are used along with helper functions.

- Generic maps currently available are:
	- BPF_MAP_TYPE_HASH, 
	- BPF_MAP_TYPE_ARRAY, 
	- BPF_MAP_TYPE_PERCPU_HASH, 
	- BPF_MAP_TYPE_PERCPU_ARRAY, 
	- BPF_MAP_TYPE_LRU_HASH, 
	- BPF_MAP_TYPE_LRU_PERCPU_HASH and 
	- BPF_MAP_TYPE_LPM_TRIE. 
- They all use the same common set of BPF helper functions in order to perform lookup, update or delete operations while implementing a different backend with differing semantics and performance characteristics.


- Non-generic maps that are currently in the kernel are:
	- BPF_MAP_TYPE_PROG_ARRAY, 
	- BPF_MAP_TYPE_PERF_EVENT_ARRAY, 
	- BPF_MAP_TYPE_CGROUP_ARRAY, 
	- BPF_MAP_TYPE_STACK_TRACE, 
	- BPF_MAP_TYPE_ARRAY_OF_MAPS, 
	- BPF_MAP_TYPE_HASH_OF_MAPS etc.

- BPF_MAP_TYPE_PROG_ARRAY is an array map which holds other BPF programs, 
- BPF_MAP_TYPE_ARRAY_OF_MAPS and BPF_MAP_TYPE_HASH_OF_MAPS both hold pointers to other maps such that entire BPF maps can be atomically replaced at runtime.




Object Pinning
--------------
- BPF maps and programs can be accessed through file descriptors, backed by anonymous inodes in the kernel.

- User space applications can make use of most file descriptor related APIs, file descriptor passing for Unix domain sockets work transparently.

- However, file descriptors are limited to a processes’ lifetime, which makes options like map sharing rather cumbersome to carry out.

- To overcome this limitation, a minimal kernel space BPF file system has been implemented, where BPF map and programs can be pinned to, a process called object pinning. 

- The BPF system call has therefore been extended with two new commands which can pin (BPF_OBJ_PIN) or retrieve (BPF_OBJ_GET) a previously pinned object.

- The BPF filesystem (/sys/fs/bpf/) supports multiple mount instances, hard and soft links etc.




Tail Calls
-----------

- Tail calls can be seen as a mechanism that allows one BPF program to call another, without returning back to the old program. Such a call has minimal overhead as unlike function calls, it is implemented as a long jump, reusing the same stack frame.

- Only programs of the same type can be tail called, and they also need to match in terms of JIT compilation, thus either JIT compiled or only interpreted programs can be invoked, but not mixed together.

- There are two components involved for carrying out tail calls: 
	- the first part needs to setup a specialized map called program array (BPF_MAP_TYPE_PROG_ARRAY) that can be populated by user space with key / values, where values are the file descriptors of the tail called BPF programs, 
	- the second part is a bpf_tail_call() helper where the context, a reference to the program array and the lookup key is passed to. Then the kernel inlines this helper call directly into a specialized BPF instruction. Such a program array is currently write-only from user space side.

- The kernel looks up the related BPF program from the passed file descriptor and atomically replaces program pointers at the given map slot. 

- When no map entry has been found at the provided key, the kernel will just “fall through” and continue execution of the old program with the instructions following after the bpf_tail_call(). 




BPF to BPF calls
----------------
- Before this feature was introduced into the kernel, a typical BPF C program had to declare any reusable code that, for example, resides in headers as always_inline such that when LLVM compiles and generates the BPF object file all these functions were inlined and therefore duplicated many times in the resulting object file, artificially inflating its code size

- The main reason why this was necessary was due to lack of function call support in the BPF program loader as well as verifier, interpreter and JITs.

- Mainstream BPF JIT compilers like x86_64 and arm64 support BPF to BPF calls today

- BPF to BPF call is an important performance optimization since it heavily reduces the generated BPF code size and therefore becomes friendlier to a CPU’s instruction cache.

- The calling convention known from BPF helper function applies to BPF to BPF calls just as well, meaning r1 up to r5 are for passing arguments to the callee and the result is returned in r0. r1 to r5 are scratch registers whereas r6 to r9 preserved across calls the usual way. 

- The maximum number of nesting calls respectively allowed call frames is 8. 

- A caller can pass pointers (e.g. to the caller’s stack frame) down to the callee, but never vice versa.


- BPF JIT compilers emit separate images for each function body and later fix up the function call addresses in the image in a final JIT pass. This has proven to require minimal changes to the JITs in that they can treat BPF to BPF calls as conventional BPF helper calls.


- There is a limit on the stack size throughout the whole call chain down to 256 bytes per subprogram.

- The BPF program’s call chain can consume at most 8KB of stack space. This limit comes from the 256 bytes per stack frame multiplied by the tail call count limit (33). Without this, the BPF programs will operate on 512-byte stack size, yielding the 16KB size in total for the maximum count of tail calls that would overflow the stack on some architectures.

- Tail call + subfunc feature is supported only on x86_64 architecture.

 
 
 


JIT
---

- The 64 bit x86_64, arm64, ppc64, s390x, mips64, sparc64 and 32 bit arm, x86_32 architectures are all shipped with an in-kernel eBPF JIT compiler, also all of them are feature equivalent and can be enabled through:

# echo 1 > /proc/sys/net/core/bpf_jit_enable

- Other architectures need to use the JIT in-kernel interpreter.

- In the kernel’s source tree, eBPF JIT support can be easily determined through issuing a grep for HAVE_EBPF_JIT.

- When using compiler, Often instructions can be mapped 1:1 with native instructions of the underlying architecture. 




Hardening
--------

- BPF locks the entire BPF interpreter image (struct bpf_prog) as well as the JIT compiled image (struct bpf_binary_header) in the kernel as read-only during the program’s lifetime in order to prevent the code from potential corruptions. 

- Any corruption happening at that point, for example, due to some kernel bugs will result in a general protection fault and thus crash the kernel instead of allowing the corruption to happen silently.

- Architectures that support setting the image memory as read-only can be determined through:
	$ git grep ARCH_HAS_SET_MEMORY | grep select

- The option CONFIG_ARCH_HAS_SET_MEMORY is not configurable, thanks to which this protection is always built-in.

- In case of /proc/sys/net/core/bpf_jit_harden set to 1 additional hardening steps for the JIT compilation take effect for unprivileged users. This effectively trades off their performance slightly by decreasing a (potential) attack surface in case of untrusted users operating on the system. 

- The decrease in program execution still results in better performance compared to switching to interpreter entirely.



- Currently, enabling hardening will blind all user provided 32 bit and 64 bit constants from the BPF program when it gets JIT compiled in order to prevent JIT spraying attacks which inject native opcodes as immediate values. 

- This is problematic as these immediate values reside in executable kernel memory, therefore a jump that could be triggered from some kernel bug would jump to the start of the immediate value and then execute these as native instructions.

- JIT constant blinding prevents this due to randomizing the actual instruction, which means the operation is transformed from an immediate based source operand to a register based one through rewriting the instruction by splitting the actual load of the value into two steps: 
	1) load of a blinded immediate value rnd ^ imm into a register, 
	2) xoring that register with rnd such that the original imm immediate then resides in the register and can be used for the actual operation.
	
	load imm ===> split into two ====> 	1) load imm ^ rnd => reg
						2) reg ^ rnd => reg=imm

- after hardening, none of the original immediate values are visible anymore in the disassembly of the second program.

- At the same time, hardening also disables any JIT kallsyms exposure for privileged users, preventing that JIT image addresses are not exposed to /proc/kallsyms anymore.

- the Linux kernel provides the option CONFIG_BPF_JIT_ALWAYS_ON which removes the entire BPF interpreter from the kernel and permanently enables the JIT compiler. 

- The kernel offers an option to disable the use of the bpf(2) system call for unprivileged users through the "/proc/sys/kernel/unprivileged_bpf_disabled" sysctl knob. This is on purpose a one-time kill switch, meaning once set to 1, there is no option to reset it back to 0 until a new kernel reboot. When set only CAP_SYS_ADMIN privileged processes out of the initial namespace are allowed to use the bpf(2) system call from that point onwards.




Offloads
--------
- Networking programs in BPF, in particular for tc and XDP do have an offload-interface to hardware in the kernel in order to execute BPF code directly on the NIC.

- Currently, the nfp driver from Netronome has support for offloading BPF through a JIT compiler which translates BPF instructions to an instruction set implemented against the NIC. 
- This includes offloading of BPF maps to the NIC as well, thus the offloaded BPF program can perform map lookups, updates and deletions.





Toolchain
==========

$ sudo apt-get install -y make gcc libssl-dev bc libelf-dev libcap-dev \
  clang gcc-multilib llvm libncurses5-dev git pkg-config libmnl-dev bison flex \
  graphviz



- LLVM is currently the only compiler suite providing a BPF back end.

- installing clang and llvm is sufficient on most recent distributions to start compiling C into BPF object files.


- The typical workflow is that BPF programs are written in C, compiled by LLVM into object / ELF files, which are parsed by user space BPF ELF loaders (such as iproute2 or others), and pushed into the kernel through the BPF system call. The kernel verifies the BPF instructions and JITs them, returning a new file descriptor for the program, which then can be attached to a subsystem (e.g. networking). If supported, the subsystem could then further offload the BPF program to hardware (e.g. NIC).

- By default, the bpf target uses the endianness of the CPU it compiles on, 

- For cross-compilation, the two targets bpfeb and bpfel were introduced, thanks to that BPF programs can be compiled on a node running in one endianness (e.g. little endian on x86) and run on a node in another endianness format (e.g. big endian on arm). 


Compiling a bpf program example:
	$ clang -O2 -Wall -target bpf -c xdp-example.c -o xdp-example.o
	# file xdp-example.o 
	xdp-example.o: ELF 64-bit LSB relocatable, eBPF, version 1 (SYSV), not stripped

Loading the program
	# ip link set dev em1 xdp obj xdp-example.o



- For debugging, clang can generate the assembler output:
	$ clang -O2 -S -Wall -target bpf -c xdp-example.c -o xdp-example.S

- Starting from LLVM’s release 6.0, there is also assembler parser support. You can program using BPF assembler directly, then use "llvm-mc" to assemble it into an object file.
	$ llvm-mc -triple bpf -filetype=obj -o xdp-example.o xdp-example.S

- Furthermore, more recent LLVM versions (>= 4.0) can also store debugging information in dwarf format into the object file. 
	$ clang -O2 -g -Wall -target bpf -c xdp-example.c -o xdp-example.o

- The llvm-objdump tool can then annotate the assembler output with the original C code used in the compilation. 
	$ llvm-objdump -S -no-show-raw-insn xdp-example.o

- Leaving out the "-no-show-raw-insn" option will also dump the raw struct bpf_insn as hex in front of the assembly.



- For LLVM IR debugging, the compilation process for BPF can be split into two steps, generating a binary LLVM IR intermediate file xdp-example.bc, which can later on be passed to llc:
	$ clang -O2 -Wall -target bpf -emit-llvm -c xdp-example.c -o xdp-example.bc
	$ llc xdp-example.bc -march=bpf -filetype=obj -o xdp-example.o

- The generated LLVM IR can also be dumped in human readable format through:
	$ clang -O2 -Wall -emit-llvm -S -c xdp-example.c -o -

- LLVM is able to attach debug information such as the description of used data types in the program to the generated BPF object file. By default this is in DWARF format.

- A heavily simplified version used by BPF is called BTF (BPF Type Format). The resulting DWARF can be converted into BTF and is later on loaded into the kernel through BPF object loaders. The kernel will then verify the BTF data for correctness and keeps track of the data types the BTF data is containing.

- BPF maps can then be annotated with key and value types out of the BTF data such that a later dump of the map exports the map data along with the related type information. This allows for better introspection, debugging and value pretty printing. 

- Any DWARF data could be converted to BTF and loaded.

- To convert DWARF --> BTF, elfutils >= 0.173 is needed.If that is not available, then adding the -mattr=dwarfris option to the llc command is required during compilation:
	$ llc -march=bpf -mattr=help |& grep dwarfris
	
- For converting DWARF into BTF, a recent pahole version (>= 1.12) is required. pahole comes with the option -J to convert DWARF into BTF from an object file. pahole can be probed for BTF support as follows (note that the llvm-objcopy tool is required for pahole as well, so check its presence, too):
	$ pahole --help | grep BTF
	  -J, --btf_encode           Encode as BTF


- Generating debugging information also requires the front end to generate source level debug information by passing -g to the clang command line. Note that -g is needed independently of whether llc’s dwarfris option is used. Full example for generating the object file:
	$ clang -O2 -g -Wall -target bpf -emit-llvm -c xdp-example.c -o xdp-example.bc
	$ llc xdp-example.bc -march=bpf -mattr=dwarfris -filetype=obj -o xdp-example.o

- Alternatively, by using clang only to build a BPF program with debugging information
	$ clang -target bpf -O2 -g -c -Xclang -target-feature -Xclang +dwarfris -c xdp-example.c -o xdp-example.o


- After successful compilation pahole can be used to properly dump structures of the BPF program based on the DWARF information:
"""
$ pahole xdp-example.o 
struct xdp_md {
	__u32                      data;                 /*     0     4 */
	__u32                      data_end;             /*     4     4 */
	__u32                      data_meta;            /*     8     4 */
	__u32                      ingress_ifindex;      /*    12     4 */
	__u32                      rx_queue_index;       /*    16     4 */

	/* size: 20, cachelines: 1, members: 5 */
	/* last cacheline: 20 bytes */
};
"""

- Through the option -J pahole can eventually generate the BTF from DWARF. In the object file DWARF data will still be retained alongside the newly added BTF data. 

- Full clang and pahole example combined:
	$ clang -target bpf -O2 -Wall -g -c -Xclang -target-feature -Xclang +dwarfris -c xdp-example.c -o xdp-example.o
	$ pahole -J xdp-example.o

	$ readelf -a xdp-example.o


- BPF loaders such as iproute2 will detect and load the BTF section, so that BPF maps can be annotated with type information.

- LLVM has a -mcpu selector for the BPF back end in order to select different versions of the BPF instruction set, namely instruction set extensions on top of the BPF base instruction set in order to generate more efficient and smaller code.
"""
$ llc -march bpf -mcpu=help
Available CPUs for this target:

  generic - Select the generic processor.
  probe   - Select the probe processor.
  v1      - Select the v1 processor.
  v2      - Select the v2 processor.
[...]
"""

- The generic processor is the default processor, which is also the base instruction set v1 of BPF. Options v1 and v2 are typically useful in an environment where the BPF program is being cross compiled and the target host where the program is loaded differs from the one where it is compiled 

- The recommended -mcpu option which is also used by Cilium internally is "-mcpu=probe"! Here, the LLVM BPF back end queries the kernel for availability of BPF instruction set extensions and when found available, LLVM will use them for compiling the BPF program whenever appropriate.

- A full command line example with llc’s -mcpu=probe:
	$ clang -O2 -Wall -target bpf -emit-llvm -c xdp-example.c -o xdp-example.bc
	$ llc xdp-example.bc -march=bpf -mcpu=probe -filetype=obj -o xdp-example.o



IMPORTANT POINTS:

- BPF programs may recursively include header file(s) with file scope inline assembly codes. The default target can handle this well, while bpf target may fail if bpf backend assembler does not understand these assembly codes, which is true in most cases.

- When compiled without -g, additional elf sections, e.g., .eh_frame and .rela.eh_frame, may be present in the object file with default target, but not with bpf target.

- The default target may turn a C switch statement into a switch table lookup and jump operation. Since the switch table is placed in the global read-only section, the bpf program will fail to load. The bpf target does not support switch table optimization. The clang option -fno-jump-tables can be used to disable switch table generation.

- For clang -target bpf, it is guaranteed that pointer or long / unsigned long types will always have a width of 64 bit, no matter whether underlying clang binary or default target (or kernel) is 32 bit. However, when native clang target is used, then it will compile these types based on the underlying architecture’s conventions, meaning in case of 32 bit architecture, pointer or long / unsigned long types e.g. in BPF context structure will have width of 32 bit while the BPF LLVM back end still operates in 64 bit.

- The native target is mostly needed in tracing for the case of walking the kernel’s struct pt_regs that maps CPU registers, or other kernel structures where CPU’s register width matters. In all other cases such as networking, the use of clang "-target bpf" is the preferred choice.


- Also, LLVM started to support 32-bit subregisters and BPF ALU32 instructions since LLVM’s release 7.0. A new code generation attribute alu32 is added. When it is enabled, LLVM will try to use 32-bit subregisters whenever possible, typically when there are operations on 32-bit types. The associated ALU instructions with 32-bit subregisters will become ALU32 instructions. "w" register, meaning 32-bit subregister, will be used instead of 64-bit "r" register.

"""
64-bit

$ clang -target bpf -emit-llvm -S 32-bit-example.c
$ llc -march=bpf 32-bit-example.ll
$ cat 32-bit-example.s
    cal:
      r1 = *(u32 *)(r1 + 0)
      r2 = *(u32 *)(r2 + 0)
      r2 += r1
      *(u32 *)(r3 + 0) = r2
      exit


32-bit

$ llc -march=bpf -mattr=+alu32 32-bit-example.ll
$ cat 32-bit-example.s
    cal:
      w1 = *(u32 *)(r1 + 0)
      w2 = *(u32 *)(r2 + 0)
      w2 += w1
      *(u32 *)(r3 + 0) = w2
      exit
"""




Writing C programs for BPF
---------------------------

When writing C programs for BPF, there are a couple of pitfalls to be aware of, compared to usual application development with C. 

1. Everything needs to be inlined, there are no function calls (on older LLVM versions) or shared library calls available.

- common library code used in BPF programs can be placed into header files and included in the main programs. 
- The use of always_inline is recommended, since the compiler could still decide to uninline large functions that are only annotated as inline.
- only BPF maps are valid relocation entries which loaders can process.

"""
#include <linux/bpf.h>

#ifndef __section
# define __section(NAME)                  \
   __attribute__((section(NAME), used))
#endif

#ifndef __inline
# define __inline                         \
   inline __attribute__((always_inline))
#endif

static __inline int foo(void)
{
    return XDP_DROP;
}

__section("prog")
int xdp_drop(struct xdp_md *ctx)
{
    return foo();
}

char __license[] __section("license") = "GPL";
"""


2. Multiple programs can reside inside a single C file in different sections.

- C programs for BPF make heavy use of section annotations. 
- A C file is typically structured into 3 or more sections. 
- BPF ELF loaders use these names to extract and prepare the relevant information in order to load the programs and maps through the bpf system
 call.
- iproute2 uses maps and license as default section name to find metadata needed for map creation and the license for the BPF program, respectively. 
- On program creation time the latter(license) is pushed into the kernel as well, and enables some of the helper functions which are exposed as GPL only in case the program also holds a GPL compatible license, for example bpf_ktime_get_ns(), bpf_probe_read() and others.
- The remaining section names are specific for BPF program code

- The code includes kernel headers, standard C headers and an iproute2 specific header containing the definition of struct bpf_elf_map. 

- iproute2 has a common BPF ELF loader and as such the definition of struct bpf_elf_map is the very same for XDP and tc typed programs.


- A struct bpf_elf_map entry defines a map in the program and contains all relevant information (such as key / value size, etc) needed to generate a map which is used from the two BPF programs. 
- The structure must be placed into the maps section, so that the loader can find it. 
- There can be multiple map declarations of this type with different variable names, but all must be annotated with __section("maps").
"""
struct bpf_elf_map acc_map __section("maps") = {
    .type           = BPF_MAP_TYPE_ARRAY,
    .size_key       = sizeof(uint32_t),
    .size_value     = sizeof(uint32_t),
    .pinning        = PIN_GLOBAL_NS,
    .max_elem       = 2,
};


static __inline int account_data(struct __sk_buff *skb, uint32_t dir)
{
    uint32_t *bytes;

    bytes = map_lookup_elem(&acc_map, &dir);
    if (bytes)
            lock_xadd(bytes, skb->len);

    return TC_ACT_OK;
}
"""


- The struct bpf_elf_map is specific to iproute2. Different BPF ELF loaders can have different formats.
- iproute2 guarantees backwards compatibility for struct bpf_elf_map.


"""
static void *BPF_FUNC(map_lookup_elem, void *map, const void *key);
"""
- Here, "map_lookup_elem()" is defined by mapping this function into the BPF_FUNC_map_lookup_elem enum value which is exposed as a helper in "uapi/linux/bpf.h". When the program is later loaded into the kernel, the verifier checks whether the passed arguments are of the expected type and re-points the helper call into a real function call.

- Moreover, "map_lookup_elem()" also demonstrates how maps can be passed to BPF helper functions. Here, "&acc_map" from the maps section is passed as the first argument to "map_lookup_elem()".


"""
#ifndef lock_xadd
# define lock_xadd(ptr, val)              \
   ((void)__sync_fetch_and_add(ptr, val))
#endif

"""

- Since the defined array map is global, the accounting needs to use an atomic operation, which is defined as lock_xadd(). 
- LLVM maps __sync_fetch_and_add() as a built-in function to the BPF atomic add instruction, that is, BPF_STX | BPF_XADD | BPF_W for word sizes.


"""
    .pinning        = PIN_GLOBAL_NS,
"""

- the "struct bpf_elf_map" tells that the map is to be pinned as PIN_GLOBAL_NS. 
- This means that tc will pin the map into the BPF pseudo file system as a node. 
- By default, it will be pinned to "/sys/fs/bpf/tc/globals/acc_map" for the given example.
- Due to the PIN_GLOBAL_NS, the map will be placed under /sys/fs/bpf/tc/globals/. "globals" acts as a global namespace that spans across object files. 
- different C files with BPF code could have the same "acc_map" definition as above with a "PIN_GLOBAL_NS" pinning. In that case, the map will be shared among BPF programs originating from various object files. 

- If "PIN_OBJECT_NS" is used, then tc would create a directory that is local to the object file. 

-  "PIN_NONE" would mean that the map is not placed into the BPF file system as a node, and as a result will not be accessible from user space after tc quits.
- tc creates two separate map instances for each program,


- Third party applications can use the "BPF_OBJ_GET" command from the bpf system call in order to create a new file descriptor pointing to the same map instance, which can then be used to lookup / update / delete map elements.

"""

$ clang -O2 -Wall -target bpf -c tc-example.c -o tc-example.o

# tc qdisc add dev em1 clsact
# tc filter add dev em1 ingress bpf da obj tc-example.o sec ingress
# tc filter add dev em1 egress bpf da obj tc-example.o sec egress

# tc filter show dev em1 ingress
filter protocol all pref 49152 bpf
filter protocol all pref 49152 bpf handle 0x1 tc-example.o:[ingress] direct-action id 1 tag c5f7825e5dac396f

# tc filter show dev em1 egress
filter protocol all pref 49152 bpf
filter protocol all pref 49152 bpf handle 0x1 tc-example.o:[egress] direct-action id 2 tag b2fd5adc0f262714

# mount | grep bpf
sysfs on /sys/fs/bpf type sysfs (rw,nosuid,nodev,noexec,relatime,seclabel)
bpf on /sys/fs/bpf type bpf (rw,relatime,mode=0700)

# tree /sys/fs/bpf/
/sys/fs/bpf/
+-- ip -> /sys/fs/bpf/tc/
+-- tc
|   +-- globals
|       +-- acc_map
+-- xdp -> /sys/fs/bpf/tc/

4 directories, 1 file

"""




3. There are no global variables allowed.

- However, there is a work-around in that the program can simply use a BPF map of type BPF_MAP_TYPE_PERCPU_ARRAY with just a single slot of arbitrary value size. 
- during execution, BPF programs are guaranteed to never get preempted by the kernel and therefore can use the single map entry as a scratch buffer for temporary data, for example, to extend beyond the stack limitation. This also functions across tail calls, since it has the same guarantees with regards to preemption.
- Otherwise, for holding state across multiple BPF program runs, normal BPF maps can be used.



4. There are no const strings or arrays allowed.

- relocation entries will be generated in the ELF file which will be rejected by loaders due to not being part of the ABI towards loaders (loaders also cannot fix up such entries as it would require large rewrites of the already compiled BPF sequence).

- Helper functions such as trace_printk() can be worked around as follows:

"""

static void BPF_FUNC(trace_printk, const char *fmt, int fmt_size, ...);

#ifndef printk
# define printk(fmt, ...)                                      \
    ({                                                         \
        char ____fmt[] = fmt;                                  \
        trace_printk(____fmt, sizeof(____fmt), ##__VA_ARGS__); \
    })
#endif

"""

- The program can then use the macro naturally like printk("skb len:%u\n", skb->len);. 
- The output will then be written to the trace pipe. 
- "tc exec bpf dbg" can be used to retrieve the messages from there.


- Use of "trace_printk()" is not recommended as Constant strings like the "skb len:%u\n" need to be loaded into the BPF stack each time the helper function is called, but also BPF helper functions are limited to a maximum of 5 arguments.

- it is recommended (for networking programs) to use the "skb_event_output()" or the "xdp_event_output()" helper, respectively.
- They allow for passing custom structs from the BPF program to the perf event ring buffer along with an optional packet sample.




5. Use of LLVM built-in functions for memset()/memcpy()/memmove()/memcmp().

- LLVM provides some built-ins that the programs can use for constant sizes (here: n) which will then always get inlined.

"""

#ifndef memset
# define memset(dest, chr, n)   __builtin_memset((dest), (chr), (n))
#endif

#ifndef memcpy
# define memcpy(dest, src, n)   __builtin_memcpy((dest), (src), (n))
#endif

#ifndef memmove
# define memmove(dest, src, n)  __builtin_memmove((dest), (src), (n))
#endif

"""

- Use of memcmp() is not recommended



6. There are no loops available (yet).

- The BPF verifier in the kernel checks that a BPF program does not contain loops by performing a depth first search of all possible program paths besides other control flow graph validations. The purpose is to make sure that the program is always guaranteed to terminate.

- A very limited form of looping is available for constant upper loop bounds by using #pragma unroll directive. Example code that is compiled to BPF:

"""

#pragma unroll
    for (i = 0; i < IPV6_MAX_HEADERS; i++) {
        switch (nh) {
        case NEXTHDR_NONE:
            return DROP_INVALID_EXTHDR;
        case NEXTHDR_FRAGMENT:
            return DROP_FRAG_NOSUPPORT;
        case NEXTHDR_HOP:
        case NEXTHDR_ROUTING:
        case NEXTHDR_AUTH:
        case NEXTHDR_DEST:
            if (skb_load_bytes(skb, l3_off + len, &opthdr, sizeof(opthdr)) < 0)
                return DROP_INVALID;

            nh = opthdr.nexthdr;
            if (nh == NEXTHDR_AUTH)
                len += ipv6_authlen(&opthdr);
            else
                len += ipv6_optlen(&opthdr);
            break;
        default:
            *nexthdr = nh;
            return len;
        }
    }

"""


- Another possibility is to use tail calls by calling into the same program again and using a "BPF_MAP_TYPE_PERCPU_ARRAY" map for having a local scratch space. This is limited to 34 iterations.





7. 
